---
title: "Final Project Summary"
author: "Arundhati G., Zhongyang H., Adarsh K., Sumanth N."
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE)
library('ezids')
```


```{r cars}
library(foreign)
library(car)
library(nnet)
library(ggplot2)
library(reshape2)
library(dplyr)
library(caret)
library(yardstick)
library(ggplot2)
```

# 1. Introduction
In recent years, California has seen record-breaking numbers of wildfires. The wildfires impact
wildlife and residents and also have serious environmental implications. These outcomes result
from heightened CO2 levels, temperatures, and precipitation. Previous exploratory analysis has shown that lower soil moisture and rainfall led to larger fires between 1992 to 2015, while lower soil moisture and rain led to more wildfires caused by equipment use. It was also found that higher rainfall resulted in a higher chance that a wildfire was caused by lightning. Government spending seemed to have a marginal impact on suppressing wildfires in California.

With climate change being an increasingly relevant issue these days, this research studies how wildfires in California have changed over time. Specifically, we focused on wildfires from 1992 to 2013 and sought to investigate factors that may have contributed to a significant impact on the intensity or volume of wildfires in California over the years. 

The first dataset we found detailed 24 years' worth of nationwide wildfires from 1992 to 2015 along with their associated sizes, causes, time of occurrence, and other metadata. We felt that analyzing every state’s wildfire patterns would be an unfocused and unfruitful effort, so we decided to focus on California, as it deals with the most wildfires compared to other states in the United States.

However, we needed more data to conduct a more in-depth analysis, so we retrieved data from Cal-Adapt, which is an effort developed by the Geospatial Innovation Facility at the University of California, Berkeley  focused on providing  data that portrays climate change in California. From here, we were able to collect data about California’s daily air temperature, soil moisture, and rainfall. From there, we were able to combine it with our wildfire data by aggregating monthly averages. We also added California’s yearly budget to our combined dataset because we were curious as to how it played a factor in the volume and intensity of wildfires over the years. Combining and cleaning the data took some work, but it resulted in all the data we wanted for our analysis.

```{r  }
library(dplyr)
data_wildfire=read.csv('final_wildfire.csv')
data_large=subset(data_wildfire, FIRE_SIZE_CLASS != 'A' & FIRE_SIZE_CLASS != 'B')
temp3=data_large %>% count(Year, DISCOVERY_DOY)
rainfall <- read.csv("rainfall_daily.csv")
soilmoisture <- read.csv("soilmoisture_daily.csv")
airtemp <- read.csv("airtemp_daily.csv")
airtemp$month <- strftime(airtemp$time, "%m")
airtemp$year <- strftime(airtemp$time, "%Y")
airtemp$DOY <- strftime(airtemp$time, "%j")
rainfall$year <- strftime(rainfall$time, "%Y")
rainfall$DOY <- strftime(airtemp$time, "%j")

cleaned_rainfall <- subset(rainfall, select = -c(time))
str(airtemp)
cleaned_airtemp <- subset(airtemp, select = -c(time))
soilmoisture$year <- strftime(rainfall$time, "%Y")
soilmoisture$DOY <- strftime(airtemp$time, "%j")

cleaned_soilmoisture <- subset(soilmoisture, select = -c(time))
joined1 <- merge(cleaned_airtemp, cleaned_soilmoisture, by.x=c("year",'DOY'), by.y=c("year",'DOY'))
joined2 <- merge(joined1, cleaned_rainfall, by.x=c("year",'DOY'), by.y=c("year",'DOY'))
joined2$year=as.integer(joined2$year)
joined2$DOY=as.integer(joined2$DOY)
```

```{r}
temp=aggregate(tair_day_livneh_vic~Year+month,data=data_wildfire,mean)
temp1=aggregate(soilmoist1_day_livneh_vic~Year+month,data=data_wildfire,mean)

temp2=aggregate(rainfall_day_livneh_vic~Year+month,data=data_wildfire,mean)

joined3=merge(temp,temp1,by.x=c('Year','month'),by.y=c('Year','month'))
joined3=merge(joined3,temp2,by.x=c('Year','month'),by.y=c('Year','month'))
temp1=data_wildfire %>% count(Year, month)

temp2=aggregate(FIRE_SIZE~Year+month,data=data_wildfire,mean)
joined4 = merge(temp2, temp1, by.x=c('Year', 'month'), by.y=c('Year', 'month'))
joined4 = merge(joined3, joined4, by.x=c('Year','month'), by.y=c('Year', 'month'))
```


# 2. Basic EDA of wildfires in California

The initial exploratory data analysis made it clear that most fires in California were correlated with high average air temperatures, low soil moisture, and periods of low rainfall. To predict factors that may cause future wildfires, the following questions need to be addressed:

* How strong is the relationship between California fires and the average rainfall, air temperature, and soil moisture? 
* Does California’s yearly fire suppression budget play any role in mitigating wildfires?
* How strongly can we predict large(class) wildfires using rainfall, air temperature, and soil moisture data? 
* How can the long-term wildfires be predicted based on the month’s temperature, soil moisture, and rainfall with reasonable accuracy?

To answer the first question, we used a correlation matrix to study the impact of temperature, soil moisture, and rainfall on the fire size. The correlation matrix showed a weak positive correlation between average air temperature and the fire size, whereas there was a strong negative correlation between fire size and soil moisture and fire size and rainfall. 
```{r results='markup'}

library(corrplot)
joined4$totalarea=joined4$n*joined4$FIRE_SIZE
cordata=joined4
colnames(cordata)[3]='AvgTemp'
colnames(cordata)[4]='AvgSoilMoisture'
colnames(cordata)[5]='AvgRainfall'
cor_fire=cor(cordata[c(3,4,5,6,7,8)])
corrplot(cor_fire,method='number',type = 'lower', diag = TRUE)
```


We studied the frequency of different fire class sizes to answer the second question. It was observed that the fire size class in California has a high frequency of class A and B fire sizes. These fire class sizes of wildfires are not considered a threat to the environment and nearby properties since they typically disappear soon. The fire class size C and higher could be dangerous as the burn size grows fast in such fire cases.  

To answer the second question, we looked at plots of California’s fire suppression budget and wildfire count over the years to see if there was anything noteworthy. Below, we can see that there is no strong relationship between the money spent controlling wildfires and the number of wildfires that occur each year. 

```{R}
budge <- read.csv("fire_suppression.csv")
budge$Budget <- as.numeric(gsub("[$,]", "", budge$Budget))
budge$Budget <- budge$Budget / 1000000

firesByYear <- data_wildfire %>% count(Year)
firesBudget <- merge(firesByYear, budge, by="Year")

ggplot(firesBudget, aes(x=Year, y=Budget)) + geom_bar(stat='identity') + ylim(0, max(budge$Budget)) + xlab("Year") +
  ylab("Budget (in Billions $)") + ggtitle("California Wildfire Suppression Budget")

ggplot(firesBudget) + geom_line(aes(x=Year, y=n), stat='identity', group=1) + xlab("Year") + ylab("Frequency") +
  ggtitle("California Fire Frequency")
```


# 3. Linear Regression of wildfires with different cause(natrual or people-caused)

First, linear modeling was used to predict the total number of fires in California, the fire size, and the burn area with respect to the environmental factors as the independent variables. These models were also looked at individually for naturally caused wildfires and those caused by people to see if there is a difference in the wildfire's impact. 

```{r }
summary_nature=read.csv('summary_nature.csv')
summary_peoplecaused=read.csv('summary_peoplecaused.csv')
```

```{r}
#Renaming variables in summary_nature
Avg_Temp <- summary_nature$tair_day_livneh_vic
Avg_SoilMoisture <- summary_nature$soilmoist1_day_livneh_vic
Avg_Rainfall <- summary_nature$rainfall_day_livneh_vic

#Renaming variables in summary_peoplecaused
Avg_Temp <- summary_peoplecaused$tair_day_livneh_vic
Avg_SoilMoisture <- summary_peoplecaused$soilmoist1_day_livneh_vic
Avg_Rainfall <- summary_peoplecaused$rainfall_day_livneh_vic
```



## 3.1 Model for all fire incidents 

```{r }
model_nnature <- lm(log(n)~ Avg_Temp + Avg_SoilMoisture, data = summary_nature)
#summary(model_nnature)
xkabledply(model_nnature, title = paste("Nature Caused:",format(formula(model_nnature)) ))
plot(model_nnature)


## Logn ~ nature variables        - PEOPLE CAUSED (P)
model_npeople <- lm(log(n)~ Avg_Temp + Avg_SoilMoisture, data = summary_peoplecaused)
#summary(model_npeople)
xkabledply(model_npeople, title = paste("People Caused:",format(formula(model_npeople)) ))
plot(model_npeople)
```

The first model with the total number of fires as the dependent variable had an adjusted R-squared value of `r format(model_nnature$r.squared *100)`%. This means that the environmental factors can explain 87% of the total variation in the number of fires when the fires were naturally caused. The R-squared value was `r format(model_npeople$r.squared *100)`% for the model where the fires were man-made. The coefficients of the independent variables showed that, for both the models, there was a direct relationship between the air temperature and the number of fires and the soil moisture and the number of fires. There was an indirect relationship between rainfall and the number of fires.

## 3.2 Linear Model for Predicting Fire Size

```{r }

## FIRE_SIZE ~ nature variables       - NATURE CAUSED (N)
model_sizenature <- lm(log(FIRE_SIZE) ~ Avg_Temp + Avg_SoilMoisture , data = summary_nature)
#summary(model_sizenature)
xkabledply(model_sizenature, title = paste("Nature Caused:",format(formula(model_sizenature)) ))
plot(model_sizenature)


model_sizepeople <- lm(log(FIRE_SIZE) ~ Avg_Temp + Avg_SoilMoisture , data = summary_peoplecaused)
#summary(model_sizepeople)
xkabledply(model_sizepeople, title = paste("People Caused:",format(formula(model_sizepeople) )))
plot(model_sizepeople)

```

Next, we modeled the environmental factors and their impact on the fire size. The environmental factors can explain `r format(model_sizenature$r.squared *100)`% of the variation in the fire size in the naturally caused fires model, whereas only `r format(model_sizepeople$r.squared *100)`% of the variation could be explained by those factors in the people caused fires model. The relationship of the independent variables was the same with the fire size as it was with the number of fires. 


## 3.3 Model for Predicting Burning Area


```{r }
model_areanature <- lm(log(FIRE_SIZE*n) ~ Avg_Temp + Avg_SoilMoisture , data = summary_nature)
#summary(model_areanature)
xkabledply(model_areanature, title = paste("Nature Caused:",format(formula(model_areanature)) ))
plot(model_areanature)


model_areapeople <- lm(log(FIRE_SIZE*n) ~ Avg_Temp + Avg_SoilMoisture , data = summary_peoplecaused)
#summary(model_areapeople)
xkabledply(model_areapeople, title = paste("People Caused:",format(formula(model_areapeople) )))
plot(model_areapeople)
```

Finally, we looked at the burn area and how environmental factors impact it. We found that `r format(model_areanature$r.squared *100)`% of the variation in burn size was explained in the naturally caused fires model, and `r format(model_areapeople$r.squared *100)`% was explained in the people caused fires model. Again, the relationship between the variables stayed consistent with previous models.  


When looking at the difference in the impact of the fires based on their cause, the linear models showed that regardless of if the wildfire was started naturally or was manmade, since the relationship between the variables remained consistent, the impact of both kinds of wildfires was equally as devastating. 


# 4.Predicting Large Fires (Class C and Above)

Through EDA, we found that the fire size class in California has a very high frequency in A and B fire class size. From the dataset, we can find that wildfires in class A and B sizes are observed every day, and those kinds of wildfires are self-limiting and are expected to disappear soon.  As previously mentioned, other kinds of wildfires fall under fire class size C or higher. Those kinds of wildfires are not typically observed frequently and could burn over a larger area. Such wildfires are a threat to the environment, wildlife, and people living in the area. The size C fire class and above are thus, considered dangerous as the burned size grows fast. So, this research focuses on the analysis of larger fires.  The class of fire with C or above appears in 4410 days in California from 1992 to 2015. We are trying to make a model that predicts the chances of getting a wildfire by given average temperature, soil moisture, and rainfall by building a logit regression model. 


In the dataset, we labeled the days of no observation with 0 and days of at least 1 observation with 1. During training, we try several configurations on input features. It is found that the model with temperature, soil moisture, and rainfall cannot be improved with temperature and soil moisture included in the model.

```{r  results='markup'}
library(dplyr)
data_wildfire=read.csv('final_wildfire.csv')
data_large=subset(data_wildfire, FIRE_SIZE_CLASS != 'A' & FIRE_SIZE_CLASS != 'B')
temp3=data_large %>% count(Year, DISCOVERY_DOY)
rainfall <- read.csv("rainfall_daily.csv")
soilmoisture <- read.csv("soilmoisture_daily.csv")
airtemp <- read.csv("airtemp_daily.csv")
airtemp$month <- strftime(airtemp$time, "%m")
airtemp$year <- strftime(airtemp$time, "%Y")
airtemp$DOY <- strftime(airtemp$time, "%j")
rainfall$year <- strftime(rainfall$time, "%Y")
rainfall$DOY <- strftime(airtemp$time, "%j")

cleaned_rainfall <- subset(rainfall, select = -c(time))
str(airtemp)
cleaned_airtemp <- subset(airtemp, select = -c(time))
soilmoisture$year <- strftime(rainfall$time, "%Y")
soilmoisture$DOY <- strftime(airtemp$time, "%j")

cleaned_soilmoisture <- subset(soilmoisture, select = -c(time))
joined1 <- merge(cleaned_airtemp, cleaned_soilmoisture, by.x=c("year",'DOY'), by.y=c("year",'DOY'))
joined2 <- merge(joined1, cleaned_rainfall, by.x=c("year",'DOY'), by.y=c("year",'DOY'))
joined2$year=as.integer(joined2$year)
joined2$DOY=as.integer(joined2$DOY)
```

```{r results='markup' }
str(joined2)

joined2=subset(joined2, year>= 1992 & year<= 2015)
joined5=joined2
joined5$DOY=joined5$DOY + 1


data_large=merge(joined2,temp3, by.x=c('year','DOY'),by.y=c('Year','DISCOVERY_DOY'),all.x=T)
data_large$n[is.na(data_large$n)]=0
data_large$fire=1
data_large$fire[data_large$n==0]=0
str(data_large)
data_large2=merge(joined5,temp3, by.x=c('year','DOY'),by.y=c('Year','DISCOVERY_DOY'),all.x=T)
data_large2$n[is.na(data_large2$n)]=0
data_large2$fire=1
data_large2$fire[data_large2$n==0]=0

for (i in range(1992,2015)){
  tdate=max(subset(data_large2,year==i)$DOY)
  data_large2$year[data_large$year == i & data_large2$DOY==tdate]=i+1
  data_large2$DOY[data_large$year == i & data_large2$DOY==tdate]=1
}
str(data_large2)

```


## 4.1 Large fire prediction model, by logit regression

```{r results='markup' }
model_large=glm(fire~tair_day_livneh_vic+soilmoist1_day_livneh_vic,data=data_large,family=binomial())
#summary(model_large)
xkabledply(model_large, title = paste("Logit Regression :",format(formula(model_large)) ))

library('pROC')
prob=predict(model_large, type = "response" )
data_large$prob=prob
h = roc(fire~prob, data=data_large)
auc(h) 
plot(h)

library("regclass")
xkabledply( confusion_matrix(model_large), title = "Confusion matrix for large fire predictor" )

```
 The model with temperature and soil moisture achieved the AUC score of 0.9017. Even though the results are high enough to predict the chances, we need to forecast the situation; this model needs the simultaneous data of input features to predict wildfire; it could be more convenient to use the model with the daily data to predict if there will be any fires the next day.  In order to increase the useability of this model, we adjust the model to predict the wildfires in the next day by merging the observations of the previous day’s environmental data. We then trained the logit model with this adjusted data set.


```{r results='markup'}

model_large=glm(fire~tair_day_livneh_vic+soilmoist1_day_livneh_vic+soilmoist1_day_livneh_vic+rainfall_day_livneh_vic,data=data_large2,family=binomial())
#summary(model_large)
xkabledply(model_large, title = paste("Predicting Probability :", format(formula(model_large))))


library('pROC')
prob=predict(model_large, type = "response" )
data_large2$prob=prob
h = roc(fire~prob, data=data_large2)
auc(h) 
plot(h)

library("regclass")
library("ezids")
xkabledply( confusion_matrix(model_large), title = "Confusion matrix for large fire predictor" )




```
```{r results='markup'}

model_large=glm(fire~tair_day_livneh_vic,data=data_large2,family=binomial())
#summary(model_large)
xkabledply(model_large, title = paste("Logit Regression :", format(formula(model_large))))


library('pROC')
prob=predict(model_large, type = "response" )
data_large2$prob=prob
h = roc(fire~prob, data=data_large2)
auc(h) 
plot(h)

library("regclass")
library("ezids")
xkabledply( confusion_matrix(model_large), title = "Confusion matrix for large fire predictor" )




```
and the model with parameters of soil moisture and temperature get an AUC of 0.9002. This AUC score is close to the model with same-day data. Next, we simplify the model by reducing the feature required in prediction. We find the model with temperature input has an AUC of 0.8799. This model is still useful for predicting wildfires the next day with fewer data points. To be specific, in the confusion matrix, the model with temperature and soil moisture input has  689 types 2 errors out of 8036 total days, and the model with only temperature input has 780 type 2 errors. It suppose that the model can predict next day situation with no significant AUC difference. 

## 4.2 Multinomial Regression Models

Next, we used multinomial regression to predict characteristics of different wildfires based on the month’s environmental conditions. This model can be used to help fire response teams in California to allocate resources whenever conditions meet certain criteria and put a location at risk for a large wildfire, which would dramatically improve the fire response time and efficiency in resource allocation.
```{r}
data_wildfire$STAT_CAUSE_CODE <- as.factor(data_wildfire$STAT_CAUSE_CODE)
data_wildfire$FIRE_SIZE_CLASS <- as.factor(data_wildfire$FIRE_SIZE_CLASS)
split <- createDataPartition(data_wildfire$FIRE_SIZE_CLASS, p = .70, list = FALSE)
train <- data_wildfire[split,]
test <- data_wildfire[-split,]
```

For these models, the independent variables were rainfall, air temperature, and soil moisture, and the dependent variable was fire size class. There are two main categorical variables of interest in this model: fire cause and fire class size. The first model produced a lower accuracy of 30%. Taking a deeper look at it, a lower accuracy score makes sense. Among 13 causes, most of the wildfires were a result of three main causes - lightning, debris burning, and “miscellaneous” with miscellaneous being the most popular cause. 27% of the wildfire records fell into this category, which is a large share compared to the other causes; lightning accounted for 14% of fires, while debris burning accounted for 7%. Other causes of wildfires were not nearly as frequent, which resulted in the model being biased towards predicting that the fires were caused by lighting, debris burning, or “miscellaneous” reasons. 

```{r}
model1 <- multinom(FIRE_SIZE_CLASS ~ month + tair_day_livneh_vic + 
                     soilmoist1_day_livneh_vic + 
                     rainfall_day_livneh_vic, data = data_wildfire)
summary(model1)
```

```{r}
# Predicting the values for train dataset
train$predictions <- as.factor(predict(model1, newdata = train, "class"))

# Building classification table
tab <- table(train$FIRE_SIZE_CLASS, train$predictions)

# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab))/sum(tab))*100,2)

cm2 <- confusionMatrix(train$predictions, train$FIRE_SIZE_CLASS)

cm2df <- as.data.frame(cm2$table)
cm2df$Prediction <- factor(cm2df$Prediction, levels=rev(levels(cm2df$Prediction)))

ggplot(cm2df, aes(Prediction,Reference, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Reference",y = "Prediction", title="Confusion Matrix for Predicted Fire Sizes")
```

```{r}
# Predicting the values for test dataset
test$predictions <- as.factor(predict(model1, newdata = test, "class"))

# Building classification table
tab <- table(test$FIRE_SIZE_CLASS, test$predictions)

# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab))/sum(tab))*100,2)

# Creating  confusion matrix
cm2 <- confusionMatrix(test$predictions, test$FIRE_SIZE_CLASS)

cm2df <- as.data.frame(cm2$table)
cm2df$Prediction <- factor(cm2df$Prediction, levels=rev(levels(cm2df$Prediction)))

# Plotting confusion matrix
ggplot(cm2df, aes(Prediction,Reference, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Reference",y = "Prediction", title="Confusion Matrix for Predicted Fire Sizes")
```
With the next model, we predicted which class fire would occur as a result of the month’s environmental conditions, and how big a fire would be if one were to happen. This model came back with a 50.9% accuracy. This accuracy score was also lower than it was expected. However, after looking into it further, this model was biased for the same reason as the previous model - a large data imbalance. In particular, class A and B fires, the smallest classes of fires, accounted for 90.87% of  fires in the dataset. In fact, due to this imbalance, the model mostly predicted Class A fires for every fire in our training dataset, resulting in predictions that were not very useful. 

```{r}
model2 <- multinom(STAT_CAUSE_CODE ~ tair_day_livneh_vic + 
                     soilmoist1_day_livneh_vic + 
                     rainfall_day_livneh_vic, data = train)
summary(model2)

# train model on training set
train$predictions <- predict(model2, newdata = train, "class")

# Create confusion matrix based on training set's accuracy
cm <- confusionMatrix(train$predictions, train$STAT_CAUSE_CODE)
cmdf <- as.data.frame(cm$table)
cmdf$Prediction <- factor(cmdf$Prediction, levels=rev(levels(cmdf$Prediction)))
tab <- table(train$STAT_CAUSE_CODE, train$predictions)
round((sum(diag(tab))/sum(tab))*100,2)

# Plot  the confusion matrix
ggplot(cmdf, aes(Reference,Prediction, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Reference",y = "Prediction", title="Confusion Matrix for Predicted Fire Causes")
```


```{r}
# Use the testing set to test our model
test$predictions <- predict(model2, newdata = test, "class")
tab <- table(test$STAT_CAUSE_CODE, test$predictions)
round((sum(diag(tab))/sum(tab))*100,2)
#  Creating confusion  matrix
cm <- confusionMatrix(test$predictions, test$STAT_CAUSE_CODE)

cmdf <- as.data.frame(cm$table)
cmdf$Prediction <- factor(cmdf$Prediction, levels=rev(levels(cmdf$Prediction)))
# Plotting confusioni matrix
ggplot(cmdf, aes(Reference,Prediction, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Reference",y = "Prediction", title="Confusion Matrix for Predicted Fire Causes")
```

# 5 Long term estimation of wildfire cases, fire size and burning area


## 5.1 predictions of Wildfire Cases, fire size and burning area 

We also wanted to evaluate the long term effect on the wildfires by nature factors.
We tried to predict the cases of wildfires, average fire size and total burned fire by nature factors.

From the corrplot in EDA part, we can find that the number of case have strong correlation with environment variable.
And the fire size, total area burned have some correlation with environment variable.
We try to use linear model for those predicted variable, the result suppose that the model is not fit well.
Then, we take log to those predicted variable. Then the models are improved.






```{r results='markup'}
#Total Fire Cases
model_case=lm(log(n)~tair_day_livneh_vic++soilmoist1_day_livneh_vic,data=joined4)
#summary(model_case)
xkabledply(model_case, title = paste("Linear Regression :", format(formula(model_case))))
VIF(model_case)
plot(model_case)

#Average Fire Size
model_avgsize=lm(log(FIRE_SIZE)~tair_day_livneh_vic++soilmoist1_day_livneh_vic,data=joined4)
#summary(model_avgsize)
xkabledply(model_avgsize, title = paste("Linear Regression :", format(formula(model_avgsize))))
VIF(model_avgsize)
plot(model_avgsize)

#Total Fire Area
model_totalarea=lm(log(totalarea)~tair_day_livneh_vic++soilmoist1_day_livneh_vic,data=joined4)
#summary(model_totalarea)
xkabledply(model_totalarea, title = paste("Linear Regression :", format(formula(model_totalarea))))
VIF(model_totalarea)
plot(model_totalarea)

```
The r-squared value for model of cases is 0.9
The r-squared value for model of average fire size 0.5377
The r-squared value for model of average total burned size with 0.7825
By the plot check we found that the cases and burned area model fit the linear model assumption well.



```{r}
temp=aggregate(tair_day_livneh_vic~Year+month,data=data_wildfire,mean)
temp1=aggregate(soilmoist1_day_livneh_vic~Year+month,data=data_wildfire,mean)

temp2=aggregate(rainfall_day_livneh_vic~Year+month,data=data_wildfire,mean)

joined3=merge(temp,temp1,by.x=c('Year','month'),by.y=c('Year','month'))
joined3=merge(joined3,temp2,by.x=c('Year','month'),by.y=c('Year','month'))
temp1=subset(data_wildfire, FIRE_SIZE_CLASS != 'A' & FIRE_SIZE_CLASS != 'B')
temp2=aggregate(FIRE_SIZE~Year+month,data=temp1,mean)
temp1=temp1 %>% count(Year, month)


joined4 = merge(temp2, temp1, by.x=c('Year', 'month'), by.y=c('Year', 'month'))
joined4 = merge(joined3, joined4, by.x=c('Year','month'), by.y=c('Year', 'month'))



```

## 5.2 The long-term large wildfire Prediction and Analysis

We build models to predict the large wildfire number firesize and total burned area.
```{r results='markup'}

library(corrplot)
joined4$totalarea=joined4$n*joined4$FIRE_SIZE
cor_fire=cor(joined4[c(3,4,5,6,7,8)])
corrplot(cor_fire,method='number',type = 'lower', diag = TRUE)

model_case=lm(log(n)~tair_day_livneh_vic++soilmoist1_day_livneh_vic,data=joined4)
#summary(model_case)
xkabledply(model_case, title = paste("Case Model:",format(formula(model_case)) ))
plot(model_case)
VIF(model_case)
model_avgsize=lm(log(FIRE_SIZE)~tair_day_livneh_vic++soilmoist1_day_livneh_vic,data=joined4)
#summary(model_avgsize)
xkabledply(model_avgsize, title = paste("Avg. Size Model Model:",format(formula(model_avgsize)) ))
plot(model_avgsize)
VIF(model_avgsize)

model_totalarea=lm(log(totalarea)~tair_day_livneh_vic++soilmoist1_day_livneh_vic,data=joined4)
#summary(model_totalarea)
xkabledply(model_totalarea, title = paste("Burn Area Model:",format(formula(model_totalarea)) ))
plot(model_totalarea)
VIF(model_totalarea)

```

We have the model that r-squared value for model of cases 0.8616
model of average fire size 0.3366
model of average total burned size with 0.7391
By the plot check we found that the model of large fire case does not fit well as the residual is not consistent.
And the model of average large fire size result shows lack of some normality from qq-plot.


# 6 Decision Tree model for MultiClass Classification

## 6.1 predicting all fire classes.


```{r results='hide'}
#install.packages("caret")
library(rpart)
library(rpart.plot)
library(caret)
library(tidyverse)
```
A decision tree model was created for multiclass classification to predict the class of future wildfires with accuracy. Firstly the insignificant variables were dropped out of the df. The variables used to train the model included "Avg_SoilMoisture", "Avg_Temp", "Avg_Rainfall", "STAT_CAUSE_CODE", and "FIRE_SIZE_CLASS". This model (4.1) was built without using any tuning parameters.

```{r  echo=FALSE}
final_wildfire=read.csv('final_wildfire.csv')
colnames(final_wildfire)[14]="Avg_Temp"
colnames(final_wildfire)[16]="Avg_SoilMoisture"
colnames(final_wildfire)[17]="Avg_Rainfall"
# createDataset
myvars <- c("Avg_SoilMoisture", "Avg_Temp", "Avg_Rainfall", "STAT_CAUSE_CODE", "FIRE_SIZE_CLASS")
temp8=final_wildfire
#subset( final_wildfire,FIRE_SIZE_CLASS != 'A' & FIRE_SIZE_CLASS != 'B')
mdb <- temp8[myvars]
```

```{r  echo=FALSE}
# createDataPartition
create_train_test <- function(data, size = 0.8, train = TRUE) {
    n_row = nrow(data)
    total_row = size * n_row
    train_sample <- 1: total_row
    if (train == TRUE) {
        return (data[train_sample, ])
    } else {
        return (data[-train_sample, ])
    }
}
```

```{r  echo=FALSE}
data_train <- create_train_test(mdb, 0.8, train = TRUE)
data_test <- create_train_test(mdb, 0.8, train = FALSE)
data_test$FIRE_SIZE_CLASS = as.factor(data_test$FIRE_SIZE_CLASS)
```
 These models have shown prominent results in trying to predict a certain class of fires. The first model while trying to predict all the fire classes did not show much promise in predicting the fires it had an accuracy of 52 percent which is slightly better than flipping a coin and trying to predict fire classes. Although accuracy was not the primary target of this model. The interest in these models comes from the structure as the models are tweaked and developed more there are many parameters and types of models that can be worked with for better models, this is evident when we tried to tune the model and managed to gain accuracy.Following is the model structure.
```{r  echo=FALSE}
# specifying the technique which will be passed into the train() function later and number parameter is the "k" in K-fold cross validation
train_control = trainControl(method = "cv", number = 5, search = "grid")

## Customsing the tuning grid (ridge regression has alpha = 0)
multi_classification_Tree_Grid =  expand.grid(maxdepth = c(1,3,5,7,9,10,11))

set.seed(50)

# Model to predict Fire class using "Avg_SoilMoisture", "Avg_Temp", "Avg_Rainfall", "STAT_CAUSE_CODE", "FIRE_SIZE_CLASS".
# training a Regression model while tuning parameters (Method = "rpart")
model = train(FIRE_SIZE_CLASS~., data = data_train, method = "rpart2", trControl = train_control, tuneGrid = multi_classification_Tree_Grid)

# summarising the results
print(model)
```
Next, we looked at the Confusion matrix and summary statistics for the model. This also found an accuracy of 52.65% for the first model. The following table of  shows the confusion matrix and summary statistics for the model. After looking at the results, it became even more evident that there are no substantial recordings of bigger fire classes like D, E, F, or G. 
Confusion matrix and Statistics.
```{r  echo=FALSE}
#use model to make predictions on test data
pred_y = predict(model, data_test)
```

## 6.2 predicting only fire classes A and B using Decision Tree.
Considering the absence of class C fires or higher, we then predicted the more frequent fire classes, that are classes A and B. We did this by building another model with the same data which was once again split in 80:20 ratio and trained this new model to see the improvement in the predictions. This model was also built without any tuning parameters modified. Following is the model structure.
```{r  echo=FALSE}
fit <- rpart(FIRE_SIZE_CLASS~., data = data_train, method = 'class')
rpart.plot(fit, extra = 106)
```

```{r  echo=FALSE}
#Prediction
predict_unseen <-predict(fit, data_test, type = 'class')
```

```{r  echo=FALSE}
table_mat <- table(data_test$FIRE_SIZE_CLASS, predict_unseen)
```

```{r  echo=FALSE}
#accuracy test
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
```

```{r  echo=FALSE}
#Tune parameters
accuracy_tune <- function(fit) {
    predict_unseen <- predict(fit, data_test, type = 'class')
    table_mat <- table(data_test$FIRE_SIZE_CLASS, predict_unseen)
    accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
    accuracy_Test
}

control <- rpart.control(minsplit = 4,
    minbucket = round(5 / 3),
    maxdepth = 3,
    cp = 0)
tune_fit <- rpart(FIRE_SIZE_CLASS~., data = data_train, method = 'class', control = control)
accuracy_tune(tune_fit)

```
Next, the confusion matrix was plotted and it was found that this model produced the same accuracy of 0.52. To further improve this model, we decided to work on the rpart.control function and tune the parameters for a better fit. Following is the confusion matrix.
```{r  echo=FALSE}
# confusion Matrix
confusionMatrix(data = predict_unseen, data_test$FIRE_SIZE_CLASS)
```
Finally, after trying different values to manipulate the fit of the model, using minsplit, to include the minimum number of observations that must exist in a node for a split to be attempted, and In any terminal leaf node, minbucket the smallest amount of observations. If just one of minbucket or minsplit is supplied, the code sets minsplit to minbucket*3 or minbucket to minbucket/3, depending on the case. ‘cp’ This parameter's primary purpose is to reduce computation time by removing splits that are clearly not worth it. In essence, the user advises the computer that any split that does not enhance the fit by cp will most likely be trimmed out by cross-validation, and that the program does not need to pursue it further. Also did a accuracy tuning fit to improve the model fit.These models may have good potential in predicting the fire classes but in the case of larger fire classes, the limitation is that most likely there will be not enough data for us to train a model and be confident in the model as of now and that would be a good thing for fires to be not frequent in bigger classes. But, in the case of predicting larger fires, we could not have good confidence in the model even after tuning.

## 6.3 predicting fire classes A and B after tuning model fit.
Furthermore we ran the prediction again with the tuned model to test the fit after the tuning. The following parameters are used for tuning the model: minsplit = 4, minbucket = round(5 / 3), maxdepth = 3, cp = 0. After predicting the tuned model produced an accuracy with the same data set. Following is the model structure.
```{r  echo=FALSE}
predict_unseen2 <-predict(tune_fit, data_test, type = 'class')
rpart.plot(tune_fit, extra = 106)
```

```{r  echo=FALSE}
# confusion Matrix
confusionMatrix(data = predict_unseen2, data_test$FIRE_SIZE_CLASS)
```
We noticed an increase in the accuracy of this model in the above confusion matrix and summary statistics plot. (4.3.2, Confusion and stats plot)We were able to predict that the smaller, and the more frequent fire classes with the best accuracy of 57% through this model. However, predicting larger fires continue to be a challenge due to the imbalance in the dataset.For larger classes, the other models would do better. When closely looking at the models through many of the variations I have tried I’ve noticed that the deeper nodes in almost all the models branch at a clause with soil moisture and air temperature. This shows that soil moisture is a strong factor when looking for fire classes, fire class in another sense is the area of fire spread and burnt, this finding is credible because with low soil moisture value the fire is easily spread on the ground even with the countermeasures are in place.
```{r , echo=FALSE}
```


# 7. Conclusion:

Through basic exploratory data analysis and modeling, this research concluded a strong relationship between the environmental factors and the wildfires, its burn area and the fire size. The linear modeling helps to show that regardless of the cause of the fire- whether by people or nature- its impact remains equally as devastating. 

Although the multinomial models were biased due to imbalanced data, we learned that smaller fires are very common across California with over 90% of them being Class A or Class B fires. If more metrics other than the ones we used can be used to predict these small fires or larger (Class C and  above), it would be a great help for fire departments across California, as they'd be able to better prepare for potential fires if conditions reach a certain point. However, to achieve this, better, more balanced data must be collected so that models can more accurately predict when  larger fires will occur. In addition, a large portion  of fires were reported as being caused by miscellaneous reasons- reporting needs to get better in order for better prediction ability. 
 

# 8. Limitations of models and Future Research: 


The raw data on the environmental factors were recorded at a daily frequency. To conduct the research, the frequency was converted into monthly and yearly. To make a more accurate prediction about the wildfire, it would have been beneficial to look at the exact data for temperature, soil moisture, and rainfall data for the wildfire instead of a monthly or yearly average. In addition, our wildfire data only went up to 2015; it would have been useful to see how frequent and intense fires from the past 7 years have been, especially with the acceleration  of global temperatures. 
 
The data we used is for the average whole California, it could not provide the variation of environment data with different latitude and longitude. Also, the model estimates the prediction for the whole california; it is not the average per specific size. The model could noe be applied with smaller region of California. 

Through our EDA and modeling, we saw that we were dealing with incredibly imbalanced data, in that most of the fires recorded by the FPA (Fire Program Analysis) System fell into only  a couple different categories. In particular, 48% of fires were caused by 3 main causes (debris burning, lightning, and miscellaneous reasons) out of 13 causes found in  the dataset. In addition, just over 90% of wildfires fell into class A and B, which means that a vast majority of recorded fires were between 0 to 9.9 acres. The classifier we trained are sensitive with the dataset balance; since most of fire cases are in size class A and B, models consistent give the
prediction in A or B can maximize the accuracy. Those models cannot be applied in prediction. In the preprocessing for the large fire predictor, we reduce the complexity of the prediction situation, and the logit models
trained from such dataset achieve a usable score with AUC around 0.9. We either need to collect more data categories for inputing into model and balance our data so that our models can more accurately predict wildfire causes and sizes.

In the future, it would also be interesting to take a deeper look into Class C fires and study the factors that cause larger fires and how it impacts the burn time of these larger fires. Lastly, we would be curious to see how the analytical and prediction techniques used to study the wildfires in California work for different states like Colorado and even the wildfires in Australia.

California’s fire suppression data was also not granular enough in that it did not detail where exactly the money was going. For the years that we could compare fire frequency to the budget, it did not look like the budget had any direct effect on the wildfires in California. As a result, we would recommend the State of California look into how the money is being spent. In addition, we believe that more specific data on how the budget is being spent will help with optimizing allocation to different efforts appropriately. 

# 9. References: 
                 


