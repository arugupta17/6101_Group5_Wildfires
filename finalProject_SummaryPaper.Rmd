---
title: "Final Project Summary"
author: "Arundhati G., Zhongyang H., Adarsh K., Sumanth N."
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('ezids')
```


```{r cars}
library(foreign)
library(car)
library(nnet)
library(ggplot2)
library(reshape2)
library(dplyr)
library(caret)
library(yardstick)
library(ggplot2)
```

# 1. Introduction
In recent years, California has seen record-breaking numbers of wildfires. The wildfires impact
wildlife and residents and also have serious environmental implications. These outcomes result
from heightened CO2 levels, temperatures, and precipitation. Previous exploratory analysis has shown that lower soil moisture and rainfall led to larger fires between 1992 to 2015, while lower soil moisture and rain led to more wildfires caused by equipment use. It was also found that higher rainfall resulted in a higher chance that a wildfire was caused by lightning. Government spending seemed to have a marginal impact on suppressing wildfires in California.

With climate change being an increasingly relevant issue these days, this research studies how wildfires in California have changed over time. Specifically, we focused on wildfires from 1992 to 2013 and sought to investigate factors that may have contributed to a significant impact on the intensity or volume of wildfires in California over the years. 

The first dataset we found detailed 24 years' worth of nationwide wildfires from 1992 to 2015 along with their associated sizes, causes, time of occurrence, and other metadata. We felt that analyzing every state’s wildfire patterns would be an unfocused and unfruitful effort, so we decided to focus on California, as it deals with the most wildfires compared to other states in the United States.

However, we needed more data to conduct a more in-depth analysis, so we retrieved data from Cal-Adapt, which is an effort developed by the Geospatial Innovation Facility at the University of California, Berkeley  focused on providing  data that portrays climate change in California.  From here, we were able to collect data about California’s daily air temperature, soil moisture, and rainfall. From there, we were able to combine it with our wildfire data by aggregating monthly averages. We also added California’s yearly budget to our combined dataset because we were curious as to how it played a factor in the volume and intensity of wildfires over the years.


# 2. Basic EDA of wildfires in California

The initial exploratory data analysis made it clear that most fires in California were correlated with high average air temperatures, low soil moisture, and periods of low rainfall. To predict factors that may cause future wildfires, the following questions need to be addressed:

* How strong is the relationship between California fires and the average rainfall, air temperature, and soil moisture? 
* Does California’s yearly fire suppression budget play any role in mitigating wildfires?
* How strongly can we predict large(class) wildfires using rainfall, air temperature, and soil moisture data? 
* How can the long-term wildfires be predicted based on the month’s temperature, soil moisture, and rainfall with reasonable accuracy?

To answer the first question, we used a correlation matrix to study the impact of temperature, soil moisture, and rainfall on the fire size. The correlation matrix showed a weak positive correlation between average air temperature and the fire size, whereas there was a strong negative correlation between fire size and soil moisture and fire size and rainfall. 



***** Insert the correlation matrix here ********




We studied the frequency of different fire class sizes to answer the second question. It was observed that the fire size class in California has a high frequency of class A and B fire sizes. These fire class sizes of wildfires are not considered a threat to the environment and nearby properties since they typically disappear soon. The fire class size C and higher could be dangerous as the burn size grows fast in such fire cases.  

To answer the second question, we looked at plots of California’s fire suppression budget and wildfire count over the years to see if there was anything noteworthy. Below, we can see that there is no strong relationship between the money spent controlling wildfires and the number of wildfires that occur each year. 

***********Insert images here************


# 3. Linear Regression of wildfires with different cause(natrual or people-caused)

First, linear modeling was used to predict the total number of fires in California, the fire size, and the burn area with respect to the environmental factors as the independent variables. These models were also looked at individually for naturally caused wildfires and those caused by people to see if there is a difference in the wildfire's impact. 

```{r }
summary_nature=read.csv('summary_nature.csv')
summary_peoplecaused=read.csv('summary_peoplecaused.csv')
```

```{r}
#Renaming variables in summary_nature
Avg_Temp <- summary_nature$tair_day_livneh_vic
Avg_SoilMoisture <- summary_nature$soilmoist1_day_livneh_vic
Avg_Rainfall <- summary_nature$rainfall_day_livneh_vic

#Renaming variables in summary_peoplecaused
Avg_Temp <- summary_peoplecaused$tair_day_livneh_vic
Avg_SoilMoisture <- summary_peoplecaused$soilmoist1_day_livneh_vic
Avg_Rainfall <- summary_peoplecaused$rainfall_day_livneh_vic
```



## 3.1 Model for all fire incidents 

```{r }
model_nnature <- lm(log(n)~ Avg_Temp + Avg_SoilMoisture, data = summary_nature)
#summary(model_nnature)
xkabledply(model_nnature, title = paste("Nature Caused:",format(formula(model_nnature)) ))
plot(model_nnature)


## Logn ~ nature variables        - PEOPLE CAUSED (P)
model_npeople <- lm(log(n)~ Avg_Temp + Avg_SoilMoisture, data = summary_peoplecaused)
#summary(model_npeople)
xkabledply(model_npeople, title = paste("People Caused:",format(formula(model_npeople)) ))
plot(model_npeople)
```

The first model with the total number of fires as the dependent variable had an adjusted R-squared value of `r format(model_nnature$r.squared *100)`%. This means that the environmental factors can explain 87% of the total variation in the number of fires when the fires were naturally caused. The R-squared value was `r format(model_npeople$r.squared *100)`% for the model where the fires were man-made. The coefficients of the independent variables showed that, for both the models, there was a direct relationship between the air temperature and the number of fires and the soil moisture and the number of fires. There was an indirect relationship between rainfall and the number of fires.

## 3.2 Model for fire size

```{r }

## FIRE_SIZE ~ nature variables       - NATURE CAUSED (N)
model_sizenature <- lm(log(FIRE_SIZE) ~ Avg_Temp + Avg_SoilMoisture , data = summary_nature)
#summary(model_sizenature)
xkabledply(model_sizenature, title = paste("Nature Caused:",format(formula(model_sizenature)) ))
plot(model_sizenature)


model_sizepeople <- lm(log(FIRE_SIZE) ~ Avg_Temp + Avg_SoilMoisture , data = summary_peoplecaused)
#summary(model_sizepeople)
xkabledply(model_sizepeople, title = paste("People Caused:",format(formula(model_sizepeople) )))
plot(model_sizepeople)

```

Next, we modeled the environmental factors and their impact on the fire size. The environmental factors can explain `r format(model_sizenature$r.squared *100)`% of the variation in the fire size in the naturally caused fires model, whereas only `r format(model_sizepeople$r.squared *100)`% of the variation could be explained by those factors in the people caused fires model. The relationship of the independent variables was the same with the fire size as it was with the number of fires. 


## 3.3 Model for burning area


```{r }
model_areanature <- lm(log(FIRE_SIZE*n) ~ Avg_Temp + Avg_SoilMoisture , data = summary_nature)
#summary(model_areanature)
xkabledply(model_areanature, title = paste("Nature Caused:",format(formula(model_areanature)) ))
plot(model_areanature)


model_areapeople <- lm(log(FIRE_SIZE*n) ~ Avg_Temp + Avg_SoilMoisture , data = summary_peoplecaused)
#summary(model_areapeople)
xkabledply(model_areapeople, title = paste("People Caused:",format(formula(model_areapeople) )))
plot(model_areapeople)
```

Finally, we looked at the burn area and how environmental factors impact it. We found that `r format(model_areanature$r.squared *100)`% of the variation in burn size was explained in the naturally caused fires model, and `r format(model_areapeople$r.squared *100)`% was explained in the people caused fires model. Again, the relationship between the variables stayed consistent with previous models.  


When looking at the difference in the impact of the fires based on their cause, the linear models showed that regardless of if the wildfire was started naturally or was manmade, since the relationship between the variables remained consistent, the impact of both kinds of wildfires was equally as devastating. 


# 4.Large fire predictor

Through EDA, we found that the fire size class in California has a very high frequency in A and B fire class size. From the dataset, we can find that wildfires in class A and B sizes are observed every day, and those kinds of wildfires are self-limiting and are expected to disappear soon.  As previously mentioned, other kinds of wildfires fall under fire class size C or higher. Those kinds of wildfires are not typically observed frequently and could burn over a larger area. Such wildfires are a threat to the environment, wildlife, and people living in the area. The size C fire class and above are thus, considered dangerous as the burned size grows fast. So, this research focuses on the analysis of larger fires.  The class of fire with C or above appears in 4410 days in California from 1992 to 2015. We are trying to make a model that predicts the chances of getting a wildfire by given average temperature, soil moisture, and rainfall by building a logit regression model. 


In the dataset, we labeled the days of no observation with 0 and days of at least 1 observation with 1. During training, we try several configurations on input features. It is found that the model with temperature, soil moisture, and rainfall cannot be improved with temperature and soil moisture included in the model. The model with temperature and soil moisture achieved the AUC score of 0.9017. Even though the results are high enough to predict the chances, we need to forecast the situation; this model needs the simultaneous data of input features to predict wildfire; it could be more convenient to use the model with the daily data to predict if there will be any fires the next day.  In order to increase the useability of this model, we adjust the model to predict the wildfires in the next day by merging the observations of the previous day’s environmental data. We then trained the logit model with this adjusted data set, and the model with parameters of soil moisture and temperature get an AUC of 0.9002. This AUC score is close to the model with same-day data. Next, we simplify the model by reducing the feature required in prediction. We find the model with temperature input has an AUC of 0.8799. This model is still useful for predicting wildfires the next day with fewer data points. To be specific, in the confusion matrix, the model with temperature and soil moisture input has  689 types 2 errors out of 8036 total days, and the model with only temperature input has 780 type 2 errors.


```{r  results='markup'}
library(dplyr)
data_wildfire=read.csv('final_wildfire.csv')
data_large=subset(data_wildfire, FIRE_SIZE_CLASS != 'A' & FIRE_SIZE_CLASS != 'B')
temp3=data_large %>% count(Year, DISCOVERY_DOY)
rainfall <- read.csv("rainfall_daily.csv")
soilmoisture <- read.csv("soilmoisture_daily.csv")
airtemp <- read.csv("airtemp_daily.csv")
airtemp$month <- strftime(airtemp$time, "%m")
airtemp$year <- strftime(airtemp$time, "%Y")
airtemp$DOY <- strftime(airtemp$time, "%j")
rainfall$year <- strftime(rainfall$time, "%Y")
rainfall$DOY <- strftime(airtemp$time, "%j")

cleaned_rainfall <- subset(rainfall, select = -c(time))
str(airtemp)
cleaned_airtemp <- subset(airtemp, select = -c(time))
soilmoisture$year <- strftime(rainfall$time, "%Y")
soilmoisture$DOY <- strftime(airtemp$time, "%j")

cleaned_soilmoisture <- subset(soilmoisture, select = -c(time))
joined1 <- merge(cleaned_airtemp, cleaned_soilmoisture, by.x=c("year",'DOY'), by.y=c("year",'DOY'))
joined2 <- merge(joined1, cleaned_rainfall, by.x=c("year",'DOY'), by.y=c("year",'DOY'))
joined2$year=as.integer(joined2$year)
joined2$DOY=as.integer(joined2$DOY)
```

```{r results='markup' }
str(joined2)

joined2=subset(joined2, year>= 1992 & year<= 2015)
joined5=joined2
joined5$DOY=joined5$DOY + 1


data_large=merge(joined2,temp3, by.x=c('year','DOY'),by.y=c('Year','DISCOVERY_DOY'),all.x=T)
data_large$n[is.na(data_large$n)]=0
data_large$fire=1
data_large$fire[data_large$n==0]=0
str(data_large)
data_large2=merge(joined5,temp3, by.x=c('year','DOY'),by.y=c('Year','DISCOVERY_DOY'),all.x=T)
data_large2$n[is.na(data_large2$n)]=0
data_large2$fire=1
data_large2$fire[data_large2$n==0]=0

for (i in range(1992,2015)){
  tdate=max(subset(data_large2,year==i)$DOY)
  data_large2$year[data_large$year == i & data_large2$DOY==tdate]=i+1
  data_large2$DOY[data_large$year == i & data_large2$DOY==tdate]=1
}
str(data_large2)

```


## 4.1 Large fire prediction model, by logit regression


```{r results='markup' }
model_large=glm(fire~tair_day_livneh_vic+soilmoist1_day_livneh_vic,data=data_large,family=binomial())
#summary(model_large)
xkabledply(model_large, title = paste("Logit Regression :",format(formula(model_large)) ))

library('pROC')
prob=predict(model_large, type = "response" )
data_large$prob=prob
h = roc(fire~prob, data=data_large)
auc(h) 
plot(h)

library("regclass")
xkabledply( confusion_matrix(model_large), title = "Confusion matrix for large fire predictor" )





```

To try to increase usability for the model, we try to make a model to predict the probability in next day:



```{r results='markup'}

model_large=glm(fire~tair_day_livneh_vic+soilmoist1_day_livneh_vic+soilmoist1_day_livneh_vic+rainfall_day_livneh_vic,data=data_large2,family=binomial())
#summary(model_large)
xkabledply(model_large, title = paste("Predicting Probability :", format(formula(model_large))))


library('pROC')
prob=predict(model_large, type = "response" )
data_large2$prob=prob
h = roc(fire~prob, data=data_large2)
auc(h) 
plot(h)

library("regclass")
library("ezids")
xkabledply( confusion_matrix(model_large), title = "Confusion matrix for large fire predictor" )




```
```{r results='markup'}

model_large=glm(fire~tair_day_livneh_vic,data=data_large2,family=binomial())
#summary(model_large)
xkabledply(model_large, title = paste("Logit Regression :", format(formula(model_large))))


library('pROC')
prob=predict(model_large, type = "response" )
data_large2$prob=prob
h = roc(fire~prob, data=data_large2)
auc(h) 
plot(h)

library("regclass")
library("ezids")
xkabledply( confusion_matrix(model_large), title = "Confusion matrix for large fire predictor" )




```
The results suppose that with temperature rainfall and soil moisture data from today; We have AUC 0.9 for predicting the the large fire in next day
If we try to make a convenient model that require only temperature to predict the probability,
the AUC is 0.88.

# 5. Multinomial Regression Models

we are try to figure out that can we predict the specific fire size
of a large wildfire(class c or above)

```{r}
data_wildfire$STAT_CAUSE_CODE <- as.factor(data_wildfire$STAT_CAUSE_CODE)
data_wildfire$FIRE_SIZE_CLASS <- as.factor(data_wildfire$FIRE_SIZE_CLASS)
temp8= subset(data_wildfire, FIRE_SIZE_CLASS != 'A' & FIRE_SIZE_CLASS != 'B')
split <- createDataPartition(temp8$FIRE_SIZE_CLASS, p = .70, list = FALSE)
train <- temp8[split,]
test <- temp8[-split,]
```

With our training data, we achieve an accuracy of 50.9% for predicting the
fire size  class of a wildfire based on that month's condition. From our
confusion matrix, it  seems that our model is biased towards predicting
that a fire is of Class A, which makes sense because smaller fires are much
more frequent than larger fires.
```{r}
model1 <- multinom(FIRE_SIZE_CLASS ~ tair_day_livneh_vic + 
                     soilmoist1_day_livneh_vic + 
                     rainfall_day_livneh_vic, data = train)
summary(model1)
train$predictions <- predict(model1, newdata = train, "class")
cm <- confusionMatrix(train$predictions, train$FIRE_SIZE_CLASS)

cmdf <- as.data.frame(cm$table)
cmdf$Prediction <- factor(cmdf$Prediction, levels=rev(levels(cmdf$Prediction)))
tab <- table(train$FIRE_SIZE_CLASS, train$predictions)
round((sum(diag(tab))/sum(tab))*100,2)

ggplot(cmdf, aes(Reference,Prediction, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Reference",y = "Prediction", title="Confusion Matrix for Predicted Fire Size")
```


```{r}
test$predictions <- predict(model1, newdata = test, "class")
tab <- table(test$FIRE_SIZE_CLASS, test$predictions)
round((sum(diag(tab))/sum(tab))*100,2)

cm <- confusionMatrix(test$predictions, test$FIRE_SIZE_CLASS)

cmdf <- as.data.frame(cm$table)
cmdf$Prediction <- factor(cmdf$Prediction, levels=rev(levels(cmdf$Prediction)))

ggplot(cmdf, aes(Reference,Prediction, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Reference",y = "Prediction", title="Confusion Matrix for Predicted Fire Size")
```
We found that since the large wildfire dataset is not balanced, the model
fails to predict class since the model strongly predict results as C class, this suppose that we need more data and try alternative model on predict such categroy.




With  our training data, the model could not predict class since it predict all of labels to majority of class.  This makes sense  because  there does not seem to be one
predominant size of the wildfires in our dataset. And we conclude this model is not useable






## 5.1 predictions of Wildfire Cases, fire size and burning area 

We also wanted to evaluate the long term effect on the wildfires by nature factors.
We tried to predict the cases of wildfires, average fire size and total burned fire by nature factors.

```{r}
temp=aggregate(tair_day_livneh_vic~Year+month,data=data_wildfire,mean)
temp1=aggregate(soilmoist1_day_livneh_vic~Year+month,data=data_wildfire,mean)

temp2=aggregate(rainfall_day_livneh_vic~Year+month,data=data_wildfire,mean)

joined3=merge(temp,temp1,by.x=c('Year','month'),by.y=c('Year','month'))
joined3=merge(joined3,temp2,by.x=c('Year','month'),by.y=c('Year','month'))
temp1=data_wildfire %>% count(Year, month)

temp2=aggregate(FIRE_SIZE~Year+month,data=data_wildfire,mean)
joined4 = merge(temp2, temp1, by.x=c('Year', 'month'), by.y=c('Year', 'month'))
joined4 = merge(joined3, joined4, by.x=c('Year','month'), by.y=c('Year', 'month'))



```

```{r results='markup'}

library(corrplot)
joined4$totalarea=joined4$n*joined4$FIRE_SIZE
cor_fire=cor(joined4[c(3,4,5,6,7,8)])
corrplot(cor_fire,method='number',type = 'lower', diag = TRUE)
```

From the corrplot, we can find that the number of case have strong correlation with environment variable.
And the fire size, total area burned have some correlation with environment variable.
We try to use linear model for those predicted variable, the result suppose that the model is not fit well.
Then, we take log to those predicted variable. Then the models are improved.






```{r results='markup'}
#Total Fire Cases
model_case=lm(log(n)~tair_day_livneh_vic++soilmoist1_day_livneh_vic,data=joined4)
#summary(model_case)
xkabledply(model_case, title = paste("Linear Regression :", format(formula(model_case))))
VIF(model_case)
plot(model_case)

#Average Fire Size
model_avgsize=lm(log(FIRE_SIZE)~tair_day_livneh_vic++soilmoist1_day_livneh_vic,data=joined4)
#summary(model_avgsize)
xkabledply(model_avgsize, title = paste("Linear Regression :", format(formula(model_avgsize))))
VIF(model_avgsize)
plot(model_avgsize)

#Total Fire Area
model_totalarea=lm(log(totalarea)~tair_day_livneh_vic++soilmoist1_day_livneh_vic,data=joined4)
#summary(model_totalarea)
xkabledply(model_totalarea, title = paste("Linear Regression :", format(formula(model_totalarea))))
VIF(model_totalarea)
plot(model_totalarea)

```
The r-squared value for model of cases is 0.9
The r-squared value for model of average fire size 0.5377
The r-squared value for model of average total burned size with 0.7825
By the plot check we found that the cases and burned area model fit the linear model assumption well.



```{r}
temp=aggregate(tair_day_livneh_vic~Year+month,data=data_wildfire,mean)
temp1=aggregate(soilmoist1_day_livneh_vic~Year+month,data=data_wildfire,mean)

temp2=aggregate(rainfall_day_livneh_vic~Year+month,data=data_wildfire,mean)

joined3=merge(temp,temp1,by.x=c('Year','month'),by.y=c('Year','month'))
joined3=merge(joined3,temp2,by.x=c('Year','month'),by.y=c('Year','month'))
temp1=subset(data_wildfire, FIRE_SIZE_CLASS != 'A' & FIRE_SIZE_CLASS != 'B')
temp2=aggregate(FIRE_SIZE~Year+month,data=temp1,mean)
temp1=temp1 %>% count(Year, month)


joined4 = merge(temp2, temp1, by.x=c('Year', 'month'), by.y=c('Year', 'month'))
joined4 = merge(joined3, joined4, by.x=c('Year','month'), by.y=c('Year', 'month'))



```

## 5.2 The long-term large wildfire Prediction and Analysis

We build models to predict the large wildfire number firesize and total burned area.
```{r results='markup'}

library(corrplot)
joined4$totalarea=joined4$n*joined4$FIRE_SIZE
cor_fire=cor(joined4[c(3,4,5,6,7,8)])
corrplot(cor_fire,method='number',type = 'lower', diag = TRUE)

model_case=lm(log(n)~tair_day_livneh_vic++soilmoist1_day_livneh_vic,data=joined4)
#summary(model_case)
xkabledply(model_case, title = paste("Case Model:",format(formula(model_case)) ))
plot(model_case)
VIF(model_case)
model_avgsize=lm(log(FIRE_SIZE)~tair_day_livneh_vic++soilmoist1_day_livneh_vic,data=joined4)
#summary(model_avgsize)
xkabledply(model_avgsize, title = paste("Avg. Size Model Model:",format(formula(model_avgsize)) ))
plot(model_avgsize)
VIF(model_avgsize)

model_totalarea=lm(log(totalarea)~tair_day_livneh_vic++soilmoist1_day_livneh_vic,data=joined4)
#summary(model_totalarea)
xkabledply(model_totalarea, title = paste("Burn Area Model:",format(formula(model_totalarea)) ))
plot(model_totalarea)
VIF(model_totalarea)

```

We have the model that r-squared value for model of cases 0.8616
model of average fire size 0.3366
model of average total burned size with 0.7391
By the plot check we found that the model of large fire case does not fit well as the residual is not consistent.
And the model of average large fire size result shows lack of some normality from qq-plot.


# 6 Decision Tree model for MultiClass Classification

## 6.1 predicting all fire classes.


```{r  echo=FALSE}
#install.packages("caret")
library(rpart)
library(rpart.plot)
library(caret)
library(tidyverse)
```
A decision tree model was created for multiclass classification to predict the class of future wildfires with accuracy. Firstly the insignificant variables were dropped out of the df. The variables used to train the model included "Avg_SoilMoisture", "Avg_Temp", "Avg_Rainfall", "STAT_CAUSE_CODE", and "FIRE_SIZE_CLASS". This model (4.1) was built without using any tuning parameters.

```{r  echo=FALSE}
final_wildfire=read.csv('final_wildfire.csv')
colnames(final_wildfire)[14]="Avg_Temp"
colnames(final_wildfire)[16]="Avg_SoilMoisture"
colnames(final_wildfire)[17]="Avg_Rainfall"
# createDataset
myvars <- c("Avg_SoilMoisture", "Avg_Temp", "Avg_Rainfall", "STAT_CAUSE_CODE", "FIRE_SIZE_CLASS")
temp8=final_wildfire
#subset( final_wildfire,FIRE_SIZE_CLASS != 'A' & FIRE_SIZE_CLASS != 'B')
mdb <- temp8[myvars]
```

```{r  echo=FALSE}
# createDataPartition
create_train_test <- function(data, size = 0.8, train = TRUE) {
    n_row = nrow(data)
    total_row = size * n_row
    train_sample <- 1: total_row
    if (train == TRUE) {
        return (data[train_sample, ])
    } else {
        return (data[-train_sample, ])
    }
}
```

```{r  echo=FALSE}
data_train <- create_train_test(mdb, 0.8, train = TRUE)
data_test <- create_train_test(mdb, 0.8, train = FALSE)
data_test$FIRE_SIZE_CLASS = as.factor(data_test$FIRE_SIZE_CLASS)
```
 Following is the model structure.
```{r  echo=FALSE}
# specifying the technique which will be passed into the train() function later and number parameter is the "k" in K-fold cross validation
train_control = trainControl(method = "cv", number = 5, search = "grid")

## Customsing the tuning grid (ridge regression has alpha = 0)
multi_classification_Tree_Grid =  expand.grid(maxdepth = c(1,3,5,7,9,10,11))

set.seed(50)

# Model to predict Fire class using "Avg_SoilMoisture", "Avg_Temp", "Avg_Rainfall", "STAT_CAUSE_CODE", "FIRE_SIZE_CLASS".
# training a Regression model while tuning parameters (Method = "rpart")
model = train(FIRE_SIZE_CLASS~., data = data_train, method = "rpart2", trControl = train_control, tuneGrid = multi_classification_Tree_Grid)

# summarising the results
print(model)
```
Next, we looked at the Confusion matrix and summary statistics for the model. This also found an accuracy of 52.65% for the first model. The following table of  shows the confusion matrix and summary statistics for the model. After looking at the results, it became even more evident that there are no substantial recordings of bigger fire classes like D, E, F, or G. 
Confusion matrix and Statistics.
```{r  echo=FALSE}
#use model to make predictions on test data
pred_y = predict(model, data_test)
```

## 6.2 predicting only fire classes A and B using Decision Tree.
Considering the absence of class C fires or higher, we then predicted the more frequent fire classes, that are classes A and B. We did this by building another model with the same data which was once again split in 80:20 ratio and trained this new model to see the improvement in the predictions. This model was also built without any tuning parameters modified. Following is the model structure.
```{r  echo=FALSE}
fit <- rpart(FIRE_SIZE_CLASS~., data = data_train, method = 'class')
rpart.plot(fit, extra = 106)
```

```{r  echo=FALSE}
#Prediction
predict_unseen <-predict(fit, data_test, type = 'class')
```

```{r  echo=FALSE}
table_mat <- table(data_test$FIRE_SIZE_CLASS, predict_unseen)
```

```{r  echo=FALSE}
#accuracy test
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
```

```{r  echo=FALSE}
#Tune parameters
accuracy_tune <- function(fit) {
    predict_unseen <- predict(fit, data_test, type = 'class')
    table_mat <- table(data_test$FIRE_SIZE_CLASS, predict_unseen)
    accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
    accuracy_Test
}

control <- rpart.control(minsplit = 4,
    minbucket = round(5 / 3),
    maxdepth = 3,
    cp = 0)
tune_fit <- rpart(FIRE_SIZE_CLASS~., data = data_train, method = 'class', control = control)
accuracy_tune(tune_fit)

```
Next, the confusion matrix was plotted and it was found that this model produced the same accuracy of 0.52. To further improve this model, we decided to work on the rpart.control function and tune the parameters for a better fit. Following is the confusion matrix.
```{r  echo=FALSE}
# confusion Matrix
confusionMatrix(data = predict_unseen, data_test$FIRE_SIZE_CLASS)
```
Finally, after trying different values to manipulate the fit of the model, using minsplit, to include the minimum number of observations that must exist in a node for a split to be attempted, and In any terminal leaf node, minbucket the smallest amount of observations. If just one of minbucket or minsplit is supplied, the code sets minsplit to minbucket*3 or minbucket to minbucket/3, depending on the case. ‘cp’ This parameter's primary purpose is to reduce computation time by removing splits that are clearly not worth it. In essence, the user advises the computer that any split that does not enhance the fit by cp will most likely be trimmed out by cross-validation, and that the program does not need to pursue it further. Also did a accuracy tuning fit to improve the model fit.

## 6.3 predicting fire classes A and B after tuning model fit.
Furthermore we ran the prediction again with the tuned model to test the fit after the tuning. The following parameters are used for tuning the model: minsplit = 4, minbucket = round(5 / 3), maxdepth = 3, cp = 0. After predicting the tuned model produced an accuracy with the same data set. Following is the model structure.
```{r  echo=FALSE}
predict_unseen2 <-predict(tune_fit, data_test, type = 'class')
rpart.plot(tune_fit, extra = 106)
```

```{r  echo=FALSE}
# confusion Matrix
confusionMatrix(data = predict_unseen2, data_test$FIRE_SIZE_CLASS)
```
We noticed an increase in the accuracy of this model in the above confusion matrix and summary statistics plot. (4.3.2, Confusion and stats plot)We were able to predict that the smaller, and the more frequent fire classes with the best accuracy of 57% through this model. However, predicting larger fires continue to be a challenge due to the imbalance in the dataset.
```{r , echo=FALSE}
```


# 7. Conclusion:

Through basic exploratory data analysis and modeling, this research concluded a strong relationship between the environmental factors and the wildfires, its burn area and the fire size. The linear modeling helps to show that regardless of the caus of the fire, its impact remains equally as devastating. 
 

# 8. Limitations and Future Research: 


The raw data on the environmental factors were recorded at a daily frequency. To conduct the research, the frequency was converted into monthly and yearly. To make a more accurate prediction about the wildfire, it would have been beneficial to look at the exact data for temperature, soil moisture, and rainfall data for the wildfire instead of a monthly or yearly average. In addition, our wildfire data only went up to 2015; it would have been useful to see how frequent and intense fires from the past 7 years have been, especially with the acceleration  of global temperatures. 

Through our EDA and modeling, we saw that we were dealing with incredibly imbalanced data, in that most of the fires recorded by the FPA (Fire Program Analysis) System fell into only  a couple different categories. In particular, 48% of fires were caused by 3 main causes (debris burning, lightning, and miscellaneous reasons) out of 13 causes found in  the dataset. In addition, just over 90% of wildfires fell into class A and B, which means that a vast majority of recorded fires were between 0 to 9.9 acres. We either need to collect more data on larger class fires or balance our data so that our models can more accurately predict wildfire causes and sizes.

In the future, it would also be interesting to take a deeper look into Class C fires and study the factors that cause larger fires and how it impacts the burn time of these larger fires. Lastly, we would be curious to see how the analytical and prediction techniques used to study the wildfires in California work for different states like Colorado and even the wildfires in Australia.

California’s fire suppression data was also not granular enough in that it did not detail where exactly the money was going. For the years that we could compare fire frequency to the budget, it did not look like the budget had any direct effect on the wildfires in California. As a result, we would recommend the State of California look into how the money is being spent. In addition, we believe that more specific data on how the budget is being spent will help with optimizing allocation to different efforts appropriately. 

# 9. References: 
                 


